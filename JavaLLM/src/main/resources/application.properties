spring.application.name=JavaLLM

# Ollama streaming configuration
langchain4j.ollama.streaming-chat-model.base-url=http://localhost:11434
langchain4j.ollama.streaming-chat-model.model-name=gemma3:4b
langchain4j.ollama.streaming-chat-model.temperature=0.7
langchain4j.ollama.streaming-chat-model.log-requests=true
langchain4j.ollama.streaming-chat-model.log-responses=true

# Regular chat configuration (optional)
langchain4j.ollama.chat-model.base-url=http://localhost:11434
langchain4j.ollama.chat-model.model-name=gemma3:4b

server.max-http-request-header-size=16KB
