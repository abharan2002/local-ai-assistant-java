spring.application.name=JavaLLM

# Ollama streaming configuration
langchain4j.ollama.streaming-chat-model.base-url=http://localhost:11434
langchain4j.ollama.streaming-chat-model.model-name=gemma3:4b
langchain4j.ollama.streaming-chat-model.temperature=0.7
langchain4j.ollama.streaming-chat-model.log-requests=true
langchain4j.ollama.streaming-chat-model.log-responses=true

# Regular chat configuration
langchain4j.ollama.chat-model.base-url=http://localhost:11434
langchain4j.ollama.chat-model.model-name=gemma3:4b

server.max-http-request-header-size=16KB
server.tomcat.connection-timeout=120000

# Google Custom Search API configuration
google.search.api.key=AIzaSyBxBGm5EON24Ibztv2kpMAYW2LotF8RU-s
google.search.engine.id=d7dc3fe8cb40a4322
